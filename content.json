[{"title":"使用Keras搭建LeNet5神经网络用于手写数字识别","date":"2020-04-28T16:53:22.000Z","path":"2020/04/29/使用Keras搭建LeNet5神经网络用于手写数字识别/","text":"用Keras搭建深度学习框架的时候，我一般都是看着别人的代码，照着敲一遍完事，感觉自己会了，但是实际上却不是这样。刚好，下周需要用钉钉讲解深度学习框架，因为对Keras相对熟悉，所以打算用Keras搭建一个最简单的LeNet-5神经网络，用于最简单的mnist手写数字的识别，也算是对之前学习知识的一个强化。 LeNet-5卷积神经网络 LeNet5网络是计算机科学家Yann LeCun于1998年发布的一篇论文《Gradient based learning applied to document-recognition》中提出的，这篇论文对于现代卷积神经网络的研究仍具有指导意义，可以说是CNN领域的第一篇经典之作。（摘自网络） 下面我将展示一下LeNet-5模型的整体框架结构： LeNet-5模式含有两个卷积层，两个池化层，两个全连接层和一个高斯连接组成。输入图像是32×32的手写数字，采用的5×5的卷积核，下采样使用的是2×2的卷积核。前期使用tanh激活函数，在最后一层全连接使用sigmoid激活函数。 接着我将展示LeNet-5网络的参数情况： 后面，我将上图原始的LeNet-5网络结构，用Keras来实现它。 MNIST手写数据集 MNIST手写数据集是学习深度学习中最常用的一个数据集。该数据集包含60,000个用于训练的示例和10,000个用于测试的示例。这些数字已经过尺寸标准化并位于图像中心，图像是固定大小(28x28像素)，其值为0到1。为简单起见，每个图像都被平展并转换为784(28 * 28)个特征的一维numpy数组。（摘自网络） 手写数据集概览如下： 现如今，几乎所有的深度学习框架都会自带mnist手写数据集，使用起来非常方便，在Keras中只需使用from keras.models import mnist这段代码就可以载入mnist数据集。 LeNet-5实现MNIST数字识别 如果想要实现一个深度学习模型，那么导入数据集，对数据集做处理，搭建模型， 训练模型，用训练集检查模型的好坏都必不可少。我会按照以上介绍的步骤一步步实现这个简单的项目。 #1.导入必须的Python包 想要实现这个项目，首先必备的工具不可少，导入必须的python包是第一步。以下是完成这个项目所必需的packages。 12345678from keras.datasets import mnistfrom keras.layers.convolutional import Conv2D, MaxPooling2Dfrom keras.layers import Flatten, Densefrom keras.models import Sequentialfrom keras.optimizers import SGDfrom sklearn.metrics import classification_reportfrom sklearn.preprocessing import LabelBinarizerimport matplotlib.pyplot as plt 以上这些包的作用我不多做介绍，下面用到的时候会做简单介绍，如果有疑问的话，请自行百度。 #2.加载数据集，并对数据做预处理 1234567891011# 加载数据集(x_train, y_train), (x_test, y_test) = mnist.load_data()# 对数据做预处理x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)# 将label进行one-hot编码lb = LabelBinarizer()y_train = lb.fit_transform(y_train)y_test = lb.transform(y_test) 使用mnist.load_data()就可以加载出数据，并将数据分为训练集合测试集。系统自带的数据集已经分好过了，不需要我们自己写代码拆分。包含60000张训练图片和10000张测试图片。 在我们将图片输入卷积神经网络之前需要对数据做一下预处理，因为卷积神经网络需要一个四维的数据，因此我们也应该将输入数据变成四维的。x_train.shape[0]表示输入数据集的数目，两个28分别表示图片的高和宽，而最后的1表示通道数，因为手写数据是黑白的，所以通道数是1，如果输入图片是彩色的，则通道数目应该改为3。 除了对输入图像做处理，我们还得对标签进行一下预处理，使用LabelBinarizer()方法将标签变成one-hot编码，方便我们后期进行分类。如果你不会这个方法的话，可以参考官方文档。 #3.搭建LeNet-5卷积神经网络 对数据集做完处理之后，接下来就应该搭建我们的框架了，代码如下： 12345678910# 搭建LeNet框架model = Sequential()model.add(Conv2D(6, (5, 5), padding='valid', activation='tanh', input_shape=(28, 28, 1)))model.add(AveragePooling2D((2, 2)))model.add(Conv2D(16, (5, 5), padding='valid', activation='tanh'))model.add(AveragePooling2D(2, 2))model.add(Flatten())model.add(Dense(120, activation='tanh'))model.add(Dense(84, activation='tanh'))model.add(Dense(10, activation='softmax')) 上面的代码，完全是按照Yann LeCun原始论文搭建，如果对于Model方法不熟悉的话，可以参看Keras的官方文档，这里不做过多阐述。 #4.对搭建的模型进行训练 12345678910# 设置优化器sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)# 对模型进行编译model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])print(model.summary())# 对模型进行训练H = model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=128, epochs=20, verbose=1, shuffle=True)model.save(\"./lenet-5-MNIST.hdf5\") 我们在对我们构建的模型进行训练之前，应该先设置优化器optimizer和损失函数loss。**优化器用来更新和计算影响模型训练和模型输出的网络参数，使其逼近或达到最优值，从而最小化(或最大化)损失函数，而损失函数用来衡量模型预测的好坏。**在这里我们使用的是SGD（随机梯度下降）优化器和categorical_crossentropy损失函数。而categorical_crossentropy损失函数常被用于多分类，如果只进行二分类的话binary_crossentropy会是更好的选择。 设置完优化器之后，接着就是要对模型进行编译操作，编译之后我们将对模型进行训练，我们将训练完成的模型用model.save()方法进行保存，方便下次直接使用。这里设置的batch_size是128，表示一个batch包含128张图片，epochs设置成20，表示将训练集训练20次。如果对.model方法不熟悉的话，可以参考我上上篇博客，这里不多做赘述。 #5.使用测试集对我们的模型进行评估 在训练完模型之后，我们需要在测试机上进行测试，以检测我们模型的好坏，代码如下： 1234# 使用测试集预测结果preds = model.predict(x_test, batch_size=128)print(classification_report(y_test.argmax(axis=1), preds.argmax(axis=1), target_names=[str(x) for x in lb.classes_])) 我们使用model.predict()方法对我们的测试集进行测试，预测的结果用preds表示，接着我们使用classification_report()来表示我们预测的准确度。使用方法请参考以下这篇博客：机器学习笔记－－classification_report&amp;精确度/召回率/F1值 #6.绘制训练时候的精确度和损失的变化 项目完成后，我们往往想看一下训练时候的Loss和Accuracy的变化，当然图片走势必比数字更直观。我参考了官方文档的方法，来对训练数据进行了可视化，代码如下： 12345678910111213141516171819# 绘制训练 &amp; 验证的准确率值plt.plot(H.history['accuracy'])plt.plot(H.history['val_accuracy'])plt.title('Model accuracy')plt.ylabel('Accuracy')plt.xlabel('Epoch')plt.legend(['Train', 'Test'], loc='upper left')plt.savefig(\"Accuracy.png\")plt.show()# 绘制训练 &amp; 验证的损失值plt.plot(H.history['loss'])plt.plot(H.history['val_loss'])plt.title('Model loss')plt.ylabel('Loss')plt.xlabel('Epoch')plt.legend(['Train', 'Test'], loc='upper left')plt.savefig(\"Loss.png\")plt.show() 上面代码是将Accuracy和Loss分开绘制的，当然你也可以将二者绘制到一起，这里参考了之前在pyimagesearch博客里编写的代码，代码如下： 123456789101112# 保存可视化训练结果plt.style.use(\"ggplot\")plt.figure()plt.plot(np.arange(0, 20), H.history[\"loss\"], label=\"train_loss\")plt.plot(np.arange(0, 20), H.history[\"val_loss\"], label=\"val_loss\")plt.plot(np.arange(0, 20), H.history[\"acc\"], label=\"train_acc\")plt.plot(np.arange(0, 20), H.history[\"val_acc\"], label=\"val_acc\")plt.title(\"Training Loss and Accuracy\")plt.xlabel(\"# Epoch\")plt.ylabel(\"Loss/Accuracy\")plt.legend()plt.savefig(\"./lenet-5-loss_acc.png\") 我在运行代码的时候，有出现过一个报错，显示KeyError:acc，我在网上搜索一番后，发现是因为Keras版本不同H.history['accuracy']写法不同，有的是写成H.history['acc']，如果你有遇到过类似错误，建议尝试二者中另外的一种，当然对应的val_accuracy也需要进行修改。 #7.完整代码 这里贴上完整代码，方便大家粘贴到编译器运行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081from keras.datasets import mnistfrom keras.layers.convolutional import Conv2D, AveragePooling2Dfrom keras.layers import Flatten, Densefrom keras.models import Sequentialfrom keras.optimizers import SGDfrom sklearn.metrics import classification_reportfrom sklearn.preprocessing import LabelBinarizerimport matplotlib.pyplot as plt# 加载数据集(x_train, y_train), (x_test, y_test) = mnist.load_data()# 对数据做预处理x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)# 将label进行one-hot编码lb = LabelBinarizer()y_train = lb.fit_transform(y_train)y_test = lb.transform(y_test)# 搭建LeNet框架model = Sequential()model.add(Conv2D(6, (5, 5), padding='valid', activation='tanh', input_shape=(28, 28, 1)))model.add(AveragePooling2D((2, 2)))model.add(Conv2D(16, (5, 5), padding='valid', activation='tanh'))model.add(AveragePooling2D(2, 2))model.add(Flatten())model.add(Dense(120, activation='tanh'))model.add(Dense(84, activation='tanh'))model.add(Dense(10, activation='softmax'))# 设置优化器，和损失函数sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)# 对模型进行编译model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])print(model.summary())# 对模型进行训练H = model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=128, epochs=20, verbose=1, shuffle=True)model.save(\"./lenet-5-MNIST.hdf5\")# 使用测试集预测结果preds = model.predict(x_test, batch_size=128)print(classification_report(y_test.argmax(axis=1), preds.argmax(axis=1), target_names=[str(x) for x in lb.classes_]))# 绘制训练 &amp; 验证的准确率值plt.plot(H.history['accuracy'])plt.plot(H.history['val_accuracy'])plt.title('Model accuracy')plt.ylabel('Accuracy')plt.xlabel('Epoch')plt.legend(['Train', 'Test'], loc='upper left')plt.savefig(\"Accuracy.png\")plt.show()# 绘制训练 &amp; 验证的损失值plt.plot(H.history['loss'])plt.plot(H.history['val_loss'])plt.title('Model loss')plt.ylabel('Loss')plt.xlabel('Epoch')plt.legend(['Train', 'Test'], loc='upper left')plt.savefig(\"Loss.png\")plt.show()\"\"\"# 保存可视化训练结果plt.style.use(\"ggplot\")plt.figure()plt.plot(np.arange(0, 20), H.history[\"loss\"], label=\"train_loss\")plt.plot(np.arange(0, 20), H.history[\"val_loss\"], label=\"val_loss\")plt.plot(np.arange(0, 20), H.history[\"acc\"], label=\"train_acc\")plt.plot(np.arange(0, 20), H.history[\"val_acc\"], label=\"val_acc\")plt.title(\"Training Loss and Accuracy\")plt.xlabel(\"# Epoch\")plt.ylabel(\"Loss/Accuracy\")plt.legend()plt.savefig(\"./lenet-5-loss_acc.png\")\"\"\" 结果和总结 这篇博客的最后，我们来看一下LeNet-5模型在mnist手写数字识别上的效果。 首先看一下在训练集和测试集上分别的精确度和损失，如下图所示： 在训练集上，精确度达到了99.38%，损失是1.97%。而在测试集上，精确度达到了99.02%，损失是3.3%。这个结果已经非常不错了，因为这是在最原始的框架上。 然后我们再看一下在测试集10000张图片上的预测结果： 在1-9的10个数字上，测试集的精确度和召回率都达到了99%，说明模型的预测效果还是非常不错的。 我们最后在看一下，我们在训练模型时Accuracy和Loss的变化趋势： 从图片中，我们可以看出在测试集的训练中，在17.5的epoch的时候，准确度和损失有点波动，但是不影响最终的结果。 以上是这篇博客的所有内容，差不多花了两三个小时写完，觉得很有成就感，感谢您的关注与阅读。","tags":[{"name":"深度学习， Keras","slug":"深度学习，-Keras","permalink":"https://www.musicpoet.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%8C-Keras/"}]},{"title":"Keras学习之ImageDataGenerator方法","date":"2020-04-27T15:30:00.000Z","path":"2020/04/27/Keras学习之ImageDataGenerator方法/","text":"ImageDataGenerator是Keras中用于图像预处理的一个方法，当我们的数据集较小时，使用这个方法可以增加我们的数据，因而使训练的结果更加准确。 这个方法当中的一些参数我一直是很蒙的状态，不知道预设这些参数后对数据产生何种影响。因此想借助官方的中文文档，加深我对这些数据的理解。老规矩，这里附上官方文档的超链接，点击图像预处理ImageDataGenerator查看。 方法的引入 首先在使用这个方法之前，应该学习如何在代码中引入，方法如下： 12import Kerasfrom keras.processing.image import ImageDataGenerator 参数 在引入ImageDataGenerator方法之后，接下来我们来看一下，其中的方法中的参数： 12345678910111213141516171819202122keras.preprocessing.image.ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None) 这个方法的参数，说实话真的很多，也侧面说明该方法的强大，下面我们挑几个常用的参数来进行解释，解释这些参数的作用与如何设置。 featurewise_center: 布尔值。将输入数据的均值设置为 0，逐特征进行。 samplewise_center: 布尔值。将每个样本的均值设置为 0。 featurewise_std_normalization: Boolean. 布尔值。将输入除以数据标准差，逐特征进行。 samplewise_std_normalization: 布尔值。将每个输入除以其标准差。 rotation_range: 整数。随机旋转的度数范围。 width_shift_range: 浮点数、一维数组或整数 float: 如果 &lt;1，则是除以总宽度的值，或者如果 &gt;=1，则为像素值。 1-D 数组: 数组中的随机元素。 int: 来自间隔 (-width_shift_range, +width_shift_range) 之间的整数个像素。 width_shift_range=2 时，可能值是整数 [-1, 0, +1]，与 width_shift_range=[-1, 0, +1] 相同；而 width_shift_range=1.0 时，可能值是 [-1.0, +1.0) 之间的浮点数。 height_shift_range: 浮点数、一维数组或整数 float: 如果 &lt;1，则是除以总宽度的值，或者如果 &gt;=1，则为像素值。 1-D array-like: 数组中的随机元素。 int: 来自间隔 (-height_shift_range, +height_shift_range) 之间的整数个像素。 height_shift_range=2 时，可能值是整数 [-1, 0, +1]，与 height_shift_range=[-1, 0, +1] 相同；而 height_shift_range=1.0 时，可能值是 [-1.0, +1.0) 之间的浮点数。 shear_range: 浮点数。剪切强度（以弧度逆时针方向剪切角度）。 zoom_range: 浮点数 或 [lower, upper]。随机缩放范围。如果是浮点数，[lower, upper] = [1-zoom_range, 1+zoom_range]。 horizontal_flip: 布尔值。随机水平翻转。 vertical_flip: 布尔值。随机垂直翻转。 rescale: 重缩放因子。默认为 None。如果是 None 或 0，不进行缩放，否则将数据乘以所提供的值（在应用任何其他转换之前）。 preprocessing_function: 应用于每个输入的函数。这个函数会在任何其他改变之前运行。这个函数需要一个参数：一张图像（秩为 3 的 Numpy 张量），并且应该输出一个同尺寸的 Numpy 张量。 validation_split: 浮点数。Float. 保留用于验证的图像的比例（严格在0和1之间）。 dtype: 生成数组使用的数据类型。 例子 使用.flow(x, y)的例子： 123456789101112131415161718（x_train, y_train), (x_test, y_test) = cifar10.load_data()y_train = np_utils.to_categorical(y_train, num_classes)y_test = np_utils.to_categorical(y_test, num_classes)datagen = ImageDataGenerator( featurewise_center=True, featurewise_std_normalization=True, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True) # 计算特征归一化所需的数量# （如果应用 ZCA 白化，将计算标准差，均值，主成分）datagen.fit(x_train)## 使用实时数据增益的批数据对模型进行拟合：model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),steps_per_epoch=len(x_train) / 32, epochs=epochs) flow 1flow(x, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None, save_to_dir=None, save_format='png', subset=None) 采集数据和标签数组，生成批量增强数据。 参数： x: 输入数据。秩为 4 的 Numpy 矩阵或元组。如果是元组，第一个元素应该包含图像，第二个元素是另一个 Numpy 数组或一列 Numpy 数组，它们不经过任何修改就传递给输出。可用于将模型杂项数据与图像一起输入。对于灰度数据，图像数组的通道轴的值应该为 1，而对于 RGB 数据，其值应该为 3。 y: 标签。 batch_size: 整数 (默认为 32)。 shuffle: 布尔值 (默认为 True)。 sample_weight: 样本权重。 seed: 整数（默认为 None）。 save_to_dir: None 或 字符串（默认为 None）。这使您可以选择指定要保存的正在生成的增强图片的目录（用于可视化您正在执行的操作）。 save_prefix: 字符串（默认 ''）。保存图片的文件名前缀（仅当 save_to_dir 设置时可用）。 save_format: “png”, “jpeg” 之一（仅当 save_to_dir 设置时可用）。默认：“png”。 subset: 数据子集 (“training” 或 “validation”)，如果 在 ImageDataGenerator 中设置了 validation_split。 返回 一个生成元组 (x, y) 的 Iterator，其中 x 是图像数据的 Numpy 数组（在单张图像输入时），或 Numpy 数组列表（在额外多个输入时），y 是对应的标签的 Numpy 数组。如果 ‘sample_weight’ 不是 None，生成的元组形式为 (x, y, sample_weight)。如果 y 是 None, 只有 Numpy 数组 x 被返回。 ImageDataGenerator方法功能很强大，这里就列出几个比较常用的，如果在实际应用中有包含没涉及到的，可以直接查看官方文档。 这里再附上一个CSDN上一位博主写的更加直观的解释，方便加深印象，附上链接可以直接点击查看。","tags":[{"name":"Keras","slug":"Keras","permalink":"https://www.musicpoet.top/tags/Keras/"}]},{"title":"Keras学习之Model方法","date":"2020-04-26T17:23:17.000Z","path":"2020/04/27/Keras学习之Model方法/","text":"在学习深度学习的时候，Keras逐渐成了我最喜欢使用的深度学习框架，因为用它搭建模型很方面，而且相比较Tensorflow来说要容易很多。 因为也是一个初学者，经常会遇到很多不会的方法，所以难免需要查询官方文档进行学习。学习任何东西都是一个从陌生到熟悉的过程，因此想在博客里新开一个tag，专门放置一些我刚遇到的还不太会使用的方法，也方便以后学习。 今天这篇博客的话，主要是学习Keras中Model这个方法的使用，文中的例子来自Keras的官方文档，如果感兴趣的话，可以直接点击蓝色的超链接进行学习。 Keras的泛型模型接口 Keras的泛型模型Model, 即广义的拥有输入和输出的模型，我们使用Model来初始化一个泛型模型： 123456from keras.models import Modelfrom keras.layers import Input, Densea = Input(shape=(32, ))b = Dense(32)(a)model = Model(input=a, output=b) 在这里，我们的模型以a为输入，以b为输出，同样我们可以构造拥有多输入和多输出的模型： 1model = Model(input=[a1, a2], output=[b1, b2, b3]) 常用的Model属性 model.layers：组成模型图的各个层 model.inputs：模型的输入张量列表 model.outputs：模型的输出张量列表 Model模型方法 compile 1compile(self, optimizer, loss, metrics=[], loss_weights=None, sample_weight_model=None) 本函数编译模型以供训练，参数有： optimizer：优化器，为预定义优化器名或优化器对象 loss：目标函数，为预定义损失函数名或者一个目标函数 metrics：列表， 包含评估模型在训练和测试时的性能指标，典型用法是metrics=['accuracy']。如果要在多输出模型中为不同的输出指定不同的指标，可向该参数传递一个字典，例如metrics={'output_a': 'accuracy'} sample_weight_mode：如果你需要按时间步为样本赋权（2D权矩阵），将该值设为“temporal”。默认为“None”，代表按样本赋权（1D权）。如果模型有多个输出，可以向该参数传入指定sample_weight_mode的字典或列表。在下面fit函数的解释中有相关的参考内容。 kwargs：使用TensorFlow作为后端请忽略该参数，若使用Theano作为后端，kwargs的值将会传递给 K.function fit 1fit(self, x, y, batch_size=32, np_epoch=10, verbose=1, callbacks=[], validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None) 本函数用以训练模型，参数有： x：输入数据。如果模型只有一个输入，那么x的类型是numpy array，如果模型有多个输入，那么x的类型应当为list，list的元素是对应于各个输入的numpy array。 y：标签， numpy array。如果模型有多个输出，可以传入一个numpy array的list。如果模型的输出拥有名字，则可以传入一个字典，将输出名与其标签对应起来。 batch_size：整数，指定进行梯度下降时每个batch包含的样本数。训练时一个batch的样本会被计算一次梯度下降，使目标函数优化一步。 np_epoch：整数，训练的轮数，训练数据将会被遍历nb_epoch次。Keras中nb开头的变量均为&quot;number of&quot;的意思。 verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录。 callbacks：list，其中的元素是keras.callbacks.Callback的对象。这个list中的回调函数将会在训练过程中的适当时机被调用。 validation_split：0~1之间的浮点数，用来指定训练集的一定比例数据作为验证集。验证集将不参与训练，并在每个epoch结束后测试的模型的指标，如损失函数、精确度等。 validation_data：形式为（X，y）或（X，y，sample_weights）的tuple，是指定的验证集。此参数将覆盖validation_spilt。 class_weight：字典，将不同的类别映射为不同的权值，该参数用来在训练过程中调整损失函数（只能用于训练）。该参数在处理非平衡的训练数据（某些类的训练样本数很少）时，可以使得损失函数对样本数不足的数据更加关注。 sample_weight：权值的numpy array，用于在训练时调整损失函数（仅用于训练）。可以传递一个1D的与样本等长的向量用于对样本进行1对1的加权，或者在面对时序数据时，传递一个的形式为（samples，sequence_length）的矩阵来为每个时间步上的样本赋不同的权。这种情况下请确定在编译模型时添加了sample_weight_mode='temporal'。 fit函数返回一个History对象，其History.history属性，记录了损失函数和其他指标的数值随epoch变化的情况，如果有验证集的话，也包含了验证集的这些指标的变化情况。 evaluate 1evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None) 本函数按batch计算在某些输入数据上模型的误差，其参数有： x：输入数据，与fit一样，是numpy array或numpy array的list。 y：标签，numpy array。 batch_size：整数，含义同fit的同名参数。 verbose：含义同fit的同名参数，但只能取0或1。 sample_weight：numpy array，含义同fit的同名参数。 本函数返回一个测试误差的标量值（如果模型没有其他评价指标），或一个标量的list（如果模型还有其他的评价指标）。model.metrics_names将给出list中各个值的含义。 predict 1predict(self, x, batch_size=32, verbose=0) 本函数按batch获得输入数据对应的输出，其参数有： 函数的返回值是预测值的numpy array fit_generator 1fit_generator(self, generator, sample_per_epoch, nb_epoch, verbose=1, callbacks=[], validation_data=None, nb_val_samples=None, class_weight=&#123;&#125;, max_q_size=10) 利用Python的生成器，逐个生成数据的batch并进行训练。生成器与模型将并行执行以提高效率。 函数的参数是： generator：生成器函数，生成器的输出应该为： 一个形如（inputs，targets）的tuple 一个形如（inputs, targets,sample_weight）的tuple。所有的返回值都应该包含相同数目的样本。生成器将无限在数据集上循环。每个epoch以经过模型的样本数达到samples_per_epoch时，记一个epoch结束 samples_per_epoch：整数，当模型处理的样本达到此数目时计一个epoch结束，执行下一个epoch verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录 validation_data：具有以下三种形式之一 生成验证集的生成器 一个形如（inputs,targets）的tuple 一个形如（inputs,targets，sample_weights）的tuple nb_val_samples：仅当validation_data是生成器时使用，用以限制在每个epoch结束时用来验证模型的验证集样本数，功能类似于samples_per_epoch max_q_size：生成器队列的最大容量 函数返回一个History对象。 一个简单的例子。 1234567891011def generate_arrays_from_file(path): while 1: f = open(path) for line in f： # create numpy array of input data # and labels, from each line in the file x, y = process_line(line) yield(x, y) f.close() model.fit_generator(generate_arrays_from_file('/my_file.txt'),samples_per_epoch=10000, nb_epoch=10) evaluate_generator 1evaluate_generator(self, generator, val_samples, max_q_size=10) 本函数使用一个生成器作为数据源，来评估模型，生成器应返回与test_on_batch的输入数据相同类型的数据。 函数的参数是： generator：生成输入batch数据的生成器 val_samples：生成器应该返回的总样本数 max_q_size：生成器队列的最大容量 nb_worker：使用基于进程的多线程处理时的进程数 pickle_safe：若设置为True，则使用基于进程的线程。注意因为它的实现依赖于多进程处理，不可传递不可pickle的参数到生成器中，因为它们不能轻易的传递到子进程中。 predict_generator 1predict_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False) 从一个生成器上获取数据并进行预测，生成器应返回与predict_on_batch输入类似的数据。 函数的参数是： generator：生成输入batch数据的生成器 val_samples：生成器应该返回的总样本数 max_q_size：生成器队列的最大容量 nb_worker：使用基于进程的多线程处理时的进程数 pickle_safe：若设置为True，则使用基于进程的线程。注意因为它的实现依赖于多进程处理，不可传递不可pickle的参数到生成器中，因为它们不能轻易的传递到子进程中。","tags":[{"name":"Keras","slug":"Keras","permalink":"https://www.musicpoet.top/tags/Keras/"}]},{"title":"Image hashing with OpenCV and Python","date":"2020-04-22T05:51:32.000Z","path":"2020/04/22/Image-hashing-with-OpenCV-and-Python/","text":"写在前面：这篇文章摘自Adrian Rosebrock的一篇技术博客，这里只做简单的翻译总结。如果想直接看原文的话，可以点击以下链接查看：Image hashing with OpenCV and Python Image hashing 或者 perceptual hashing 的过程包括： 检测一张图像的内容 根据输入图像的内容，为它创建一个特殊的hash值 或许最出名的Image hashing 工具/服务是TinEye，它是一个逆向的图像搜索引擎。 使用TinEye, 用户可以： 上传一张图片 TinEye会告诉用户网上这张图片的出处 在这节的开头，你可以看到一个可视化的perceptual hashing/image hashing的例子。 对于一张给定的输入图像，我们的算法会根据图像的视觉表现来计算图片的hash值。同时外观相似的图像，也应该有尽可能相似的hash值。（这里相似是指hash值之间的Hamming距离） 通过使用image hashing 算法，我们可以在一定的时间内找到近似的图像。其中最差的情况是，我们需要遍历整个数据结构，时间复杂度是O(lg n)。 这里做个提醒，通过这篇文章我们会： 讨论image hashing/perceptual hashing(和为什么传统的hash不奏效) 实现image hashing， 特别是difference hashing(dHash) 使用image hashing来解决现实世界中的问题 Step #1: 将输入图像转化为灰度图 我们image hashing算法的第一步是将输入的图像转化为灰度图，同时舍弃所有的颜色信息。 舍弃颜色信息可以让我们： 对图像进行hash的过程变得更快，因为我们只需要检测一个通道 匹配那些相同但是颜色空间稍有出入的图像（因为颜色信息已经被我们去除了） 如果无论如何你都对颜色特别感兴趣，你可以将image hashing 算法分别运用在每个通道上，然后最后将结果结合起来。（尽管这会导致三倍大的hash) Step #2: 修改原始图像的大小 由于我们的输入图像已经被转换成灰度图了，我们需要忽略横纵比，将它压缩到9×8的像素。对于大多数的图像和数据集来说，修改原始图片大小的步骤是这个算法最慢的一步。 然而，现在的你可能有两个问题： 我们在修改原始图像大小的时候为什么要忽略横纵比？ 为什么是9×8的像素——这似乎是一个很奇怪的大小？ 首先回答第一个问题： 我们将图像压缩到9×8的大小同时忽略横纵比，是为了确保image hash的结果可以匹配相似的图像而不管它们初始的空间维度。 第二个问题需要更多的解释，我们会在下一步进行解答。 Step #3: 计算图片之间的差异 我们最后的目标是计算一个64-bit的hash——因为8×8=64，十分接近我们目标。 因此，为什么要将图像的大小修改为9×8呢？ 请牢记我们需要实现算法的名字：difference hash。Difference hash算法通过计算相邻像素的差异而生效。 如果我们使用一张每行有9个像素的图作为输入图像， 然后计算相邻列的像素，我们最后会得到8个不同结果。8行就会产生64个不同的结果，而这个结果也是我们希望编程的64位的hash. 事实上，我们并不一定要计算差异——我们可以使用更好的方法来测试。 如果你对这点仍有疑惑，不用担心， 一旦我们开始看一些代码之后，相信一切都会变得清晰起来。 Step #4: 建立hash 最后一步是分配bit值按后建立结果hash。为了实现这个目标，我们使用一个简单的二分类测试。 给定一张差异图像D，设其对应的像素为P，我们使用下面的测试：P[X] &gt; P[X + 1] = 1 else 0。 在这个例子中，我们测试左边的像素是不是比右边的像素更亮。如果左边像素更亮的话，我们就将输出值设为1。否则，如果左边的像素更暗的话，我们就将输出值设为0. 输出的图像如下图所示。 使用Difference hash的好处 使用difference hash 有很多好处，主要包含以下几点： 如果我们输入图像的横纵比改变的话，我们的image hash也不会改变 调整亮度或者对比度：1)不会改变我们的hash值 2）或者只是有轻微改变，以确保hash值彼此尽可能贴近 Difference hash非常快 以下是代码部分： 1234567# import the necessary packagesfrom imutils import pathsimport argparseimport timeimport sysimport cv2import os 12345678910# define the difference hash function def dhash(image, hashSize=8): # resize the input image, adding a single column (width) so we # can compute the horizontal gradient resized = cv2.resize(image, (hashSize + 1, hashSize)) # compute the (relative) horizontal gradient between adjacent # column pixels diff = resized[:, 1:] &gt; resized[:, :-1] # convert the difference image to a hash return sum([2 ** i for (i, v) in enumerate(diff.flatten()) if v]) 1234567# construct the argument parse and parse the argumentsap = argparse.ArgumentParser()ap.add_argument(\"-a\", \"--haystack\", required=True, help=\"dataset of images to search through (i.e., the haytack)\")ap.add_argument(\"-n\", \"--needles\", required=True, help=\"set of images we are searching for (i.e., needles)\")args = vars(ap.parse_args()) 123456789# grab the paths to both the haystack and needle images print(\"[INFO] computing hashes for haystack...\")haystackPaths = list(paths.list_images(args[\"haystack\"]))needlePaths = list(paths.list_images(args[\"needles\"]))# remove the `\\` character from any filenames containing a space# (assuming you're executing the code on a Unix machine)if sys.platform != \"win32\": haystackPaths = [p.replace(\"\\\\\", \"\") for p in haystackPaths] needlePaths = [p.replace(\"\\\\\", \"\") for p in needlePaths] 123456# grab the base subdirectories for the needle paths, initialize the# dictionary that will map the image hash to corresponding image,# hashes, then start the timerBASE_PATHS = set([p.split(os.path.sep)[-2] for p in needlePaths])haystack = &#123;&#125;start = time.time() 123456789101112131415# loop over the haystack pathsfor p in haystackPaths: # load the image from disk image = cv2.imread(p) # if the image is None then we could not load it from disk (so # skip it) if image is None: continue # convert the image to grayscale and compute the hash image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) imageHash = dhash(image) # update the haystack dictionary l = haystack.get(imageHash, []) l.append(p) haystack[imageHash] = l 12345# show timing for hashing haystack images, then start computing the# hashes for needle imagesprint(\"[INFO] processed &#123;&#125; images in &#123;:.2f&#125; seconds\".format( len(haystack), time.time() - start))print(\"[INFO] computing hashes for needles...\") 123456789101112131415161718192021# loop over the needle pathsfor p in needlePaths: # load the image from disk image = cv2.imread(p) # if the image is None then we could not load it from disk (so # skip it) if image is None: continue # convert the image to grayscale and compute the hash image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) imageHash = dhash(image) # grab all image paths that match the hash matchedPaths = haystack.get(imageHash, []) # loop over all matched paths for matchedPath in matchedPaths: # extract the subdirectory from the image path b = p.split(os.path.sep)[-2] # if the subdirectory exists in the base path for the needle # images, remove it if b in BASE_PATHS: BASE_PATHS.remove(b) 12345# display directories to checkprint(\"[INFO] check the following directories...\")# loop over each subdirectory and display itfor b in BASE_PATHS: print(\"[INFO] &#123;&#125;\".format(b))","tags":[{"name":"CV","slug":"CV","permalink":"https://www.musicpoet.top/tags/CV/"}]},{"title":"OOP设计7大设计原则总结","date":"2020-04-09T09:07:24.000Z","path":"2020/04/09/OOP设计7大设计原则总结/","text":"在软件开发中，为了提高软件系统的可维护性和可复用性，增加软件的可扩展性和灵活性，程序员要尽量根据 7 条原则来开发程序，从而提高软件开发效率、节约软件开发成本和维护成本。下面将为你依次介绍这7条开发原则。 一. 开闭原则 定义 软件实体应当对扩展开放，对修改关闭。当应用需求改变时，在不修改软件实体源代码或者二进制代码的前提下，可以扩展模块的功能，使其满足新的需求。 作用 对软件测试的影响。软件遵循开闭原则的话，软件测试只需要对扩展的代码进行测试就好了，因为原来的测试代码可以正常运行。 可以提高代码的可复用性。粒度越小，被复用的可能性就越大；在面向对象的程序设计中，根据原子和抽象编程可以提高代码的可复用性。 可以提高代码的可维护性。遵循开闭原则的软件，其稳定性高和延续性强，从而易于扩展和维护。 实现方法 可以通过“抽象约束、封装变化”来实现开闭原则。即通过接口或者抽象类为软件实体定义一个相对稳定的抽象层，从而将相同的可变因素封装在相同的具体实现类中。 因为抽象灵活性好、适应性广，只要抽象地合理，可以基本保持软件架构的稳定。而软件中容易改变的细节可以从抽象派生的实现类来进行扩展，当软件需要发生变化时，只需要根据需求重新派生一个实现类来扩展就可以了。 二.里氏代换原则（LSP原则） 定义 继承必须确保超类所拥有的性质在子类中仍然成立。它是继承复用的基础，也反映了基类与子类之间的关系，是对开闭原则的补充，是对实现抽象化的具体步骤的规范。 作用 里氏替换原则是实现开闭原则的重要方式之一。 它克服了继承中重写父类造成的可复用性变差的缺点。 它是动作正确性的保证。即类的扩展不会给已有的系统引入新的错误，降低了代码出错的可能性。 实现方法 里氏替换原则通俗来说就是：**子类可以扩展父类的功能，但是不能改变父类原有的功能。**子类继承父类时，除了添加新的方法完成新增的功能外，尽量不要重写父类的方法。 如果通过重写父类的方法来完成新的功能，这样写起来虽然简单，但是整个继承体系的复用性会变差，特别是运行多态比较频繁时，程序运行出错的概率会非常大。 如果程序违背了里氏替换原则，继承类的对象在基类出现的地方那个会出现运行错误。修正方法是：取消原来的继承关系，重新设计它们之间的关系。 三. 依赖倒置原则 定义 高层模块不应该依赖低层模块，两者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象。 核心思想：要面向接口编程，不要面向实现编程。 依赖倒置原则是实现开闭原则的重要途径之一，它降低了客户与实现模块之间的耦合。而使用接口或者抽象类的目的是制定好规范和契约，而不去涉及任何具体的操作，把展现细节的任务交给它们的实现类去完成。 作用 可以降低类之间的耦合性。 可以提高系统的稳定性。 可以减少并行开发引起的风险。 可以提高代码的可读性和可维护性。 实现方法 我们在具体编程中只要遵循以下4点，就能在项目中满足这个规则： 每个类尽量提供接口或抽象类，或者两者都具备。 变量的声明类型尽量是接口或者抽象类。 任何类都不应该从具体类派生。 使用继承时尽量遵循里氏替换原则。 四. 单一职责原则 定义 一个类应该有且仅有一个引起它变化的原因，否则类应该被拆分。 该原则提出对象不应该承担太多职责，如果一个对象承担太多的职责，至少会有以下两个缺点： 一个职责的变化可能会削弱或者抑制这个类实现其他职责的能力。 当客户端需要该对象的某一个职责时，不得不将其他不需要的职责全都包含进来，从而造成冗余代码或代码的浪费。 作用 降低类的复杂度。一个类只负责一项职责，其逻辑肯定要比负责多项职责简单得多。 提高类的可读性。复杂性降低，自然其可读性会提高。 提高系统的可维护性。可读性提高，那自然更容易维护了。 实现方法 单一职责原则是最简单但又最难运用的原则，需要设计人员发现类的不同职责并将其分离，再封装到不同的类或模块中。而发现类的多重职责需要设计人员具有较强的分析设计能力和相关重构经验。 五.接口隔离原则(ISP) 定义 程序员应尽量将臃肿庞大的接口拆分成更小和更具体的接口，让接口只包含客户感兴趣的方法。一个类对另一个类的依赖应该建立在最小的接口之上。 接口隔离原则和单一职责原则的区别 单一职责原则注重的是职责，而接口隔离原则注重的是对接口依赖的隔离。 单一职责原则主要是约束类，它针对的是程序中的实现和细节；接口隔离原则主要是约束接口，主要针对抽象和程序整体框架的构建。 作用 将臃肿庞大的接口分解为多个粒度小的接口，可以预防外来变更的扩散，提高系统的灵活性和可维护性。 接口隔离提高了系统的内聚性，减少了对外交互，降低了系统的耦合性。 如果接口的粒度大小定义合理，能够保证系统的稳定性；但是，如果定义过小，则会造成接口数量过多，使设计复杂化；如果定义太大，灵活性降低，无法提供定制服务，给整体项目带来无法预料的风险。 使用多个专门的接口还能够体现对象的层次，因为可以通过接口的继承，实现对总接口的定义。 能减少项目工程中的代码冗余。 实现方法 接口尽量小，但是要有限度。一个接口只服务于一个子模块或业务逻辑。 为依赖接口的类定制服务。只提供调用者需要的方法，屏蔽不需要的方法。 了解环境，拒绝盲从。每个项目或产品都有选定的环境因素，环境不同，接口拆分的标准就不同深入了解业务逻辑。 提高内聚，减少对外交互。使接口用最少的方法去完成最多的事情。 六.迪米特法则 定义 只与你“直接朋友”交谈，不跟“陌生人”说话。 迪米特法则中的“朋友”是指：当前对象本身、当前对象的成员对象、当前对象所创建的对象、当前对象的方法参数等，这些对象同当前对象存在关联、聚合或组合关系，可以直接访问这些对象的方法。 如果两个软件实体无需直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。 作用 降低了类之间的耦合度，提高了模块的相对独立性。 由于亲合度降低，从而提高了类的可复用率和系统的扩展性。 注：过度使用迪米特法则会使系统产生大量的中介类，从而增加系统的复杂性，使模块之间的通信效率降低。 实现方法 从依赖者的角度来说，只依赖应该依赖的对象。 从被依赖者的角度说，只暴露应该暴露的方法。 使用迪米特法则应注意的点 在类的划分上，应该创建弱耦合的类。类与类之间的耦合越弱，就越有利于实现可复用的目标。 在类的结构设计上，尽量降低类成员的访问权限。 在类的设计上，优先考虑将一个类设置成不变类。 在对其他类的引用上，将引用其他对象的次数降到最低。 不暴露类的属性成员，而应该提供相应的访问器（set 和 get 方法）。 谨慎使用序列化（Serializable）功能。 七.合成复用原则(CRP) 定义 在软件复用时，要尽量先使用组合或者聚合等关联关系来实现，其次才考虑使用继承关系来实现。如果要使用继承关系，必须得严格遵循里氏替换原则。 作用及重要性 通常类的复用分为继承复用和合成复用两种。继承复用虽然简单易实现，但是它也有很多缺点： 继承复用破坏了类的封装性。因为继承会将父类的实现细节暴露给子类，父类对子类是透明的，所以这种复用又称为“白箱”复用。 子类与父类的耦合度高。父类的实现的任何改变都会导致子类的实现发生变化，这不利于类的扩展与维护。 它限制了复用的灵活性。从父类继承而来的实现是静态的，在编译时已经定义，所以在运行时不可能发生变化。 采用组合或者聚合复用时，则可避免这些缺点，同时也具备以下优点： 维持了类的封装性。因为成分对象的内部细节是新对象看不见的，所以这种复用又称为“黑箱”复用。 新旧类之间的耦合度低。这种复用所需的依赖较少，新对象存取成分对象的唯一方法是通过成分对象的接口。 复用的灵活性高。这种复用可以在运行时动态进行，新对象可以动态地引用与成分对象类型相同的对象。 3.实现方法 通过将已有的对象纳入新对象中，作为新对象的成员对象来实现的，新对象可以调用已有对象的功能，从而达到复用。 八.总结 以上介绍的7种设计原则是软件设计模式必须遵循的原则，各种原则要求的侧重点不同。 开闭原则是总纲，它告诉我们要对扩展开放，对修改关闭。 里氏替换原则告诉我们不要破坏继承体系。 依赖倒置原则告诉我们要面向接口编程。 单一职责原则告诉我们实现类要职责单一。 接口隔离原则告诉我们在设计接口的时候要精简单一。 迪米特法则告诉我们要降低耦合度。 合成复用原则告诉我们要优先使用组合或者聚合关系复用，少用继承关系复用。 以上内容整理自 C语言中文网 ，感谢你的阅读。","tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://www.musicpoet.top/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"针对Github下载速度过慢的解决办法","date":"2020-03-25T14:29:42.000Z","path":"2020/03/25/针对Github下载速度过慢的解决办法/","text":"对于一个程序员来说，逛Github可以说是一件必不可少的事情，毕竟这是全球最大的~~“同性交友网站”~~程序员交流网站。我们也会经常看到一些感兴趣的项目，于是便想着下载下来研究。经常困扰我们的一个问题就是，从Github上下载项目实在是太慢了，每秒十几kb的下载速度让人抓狂，实在是太太太太太慢了。于是便想着有没有什么办法可以提高下载速度，从网上搜索了一番，发现了可以借助**”码云“**间接下载Github上的项目。 接下来，我就简单介绍一下用&quot;码云&quot;下载Github项目的流程。 1.注册码云 首先简单介绍一下&quot;码云&quot;这个网站吧，它是码云是开源中国社区2013年推出的基于 Git 的代码托管服务，目前已经成为国内最大的代码托管平台，致力于为国内开发者提供优质稳定的托管服务。(摘自官方介绍) 简单来说，它和Github的性质是一样的，只是它的服务器在国内，访问和下载速度会相对快很多。 首先贴上”码云“官方网站的链接：码云 。 打开网站之后，在网站右上角有&quot;注册&quot;按钮，鼠标点击进行注册。 注册的时候，尽量使用英文的用户姓名，并且英文名和你的个人码云的地址有关。同时在注册的时候需要验证手机或者邮箱。 2.将你需要下载的仓库，导入自己的码云中 用注册的账户登录之后，你会看到如下操作界面。在界面的左下角有一个“仓库”的区间，点击**&quot;+&quot;**进行仓库的添加。 &quot;+&quot;点击完毕后，在跳出的界面往下拉，在页面底端会有一个&quot;导入已有仓库&quot;的链接，点击之后在输入框中贴上自己想要导入的库，然后点击创建。 导入数据需要花点时间，过了一段时间，系统会提示你导入成功，你会看到你导入的库。点击右上方的&quot;克隆/下载&quot;按钮，便可进行克隆和下载。 你可以下载ZIP格式的压缩文件，也可以通过复制HTTPS的文件，通过git进行下载。 下载的速度比直接用Github下载快很多，妈妈再也不用担心下大型的项目要花很长时间了。下载速度可以达到3MB每秒，可以说速度还可以了。 以上教程来自网络，这里只做记录分享，感谢您的阅读。","tags":[{"name":"技术问题","slug":"技术问题","permalink":"https://www.musicpoet.top/tags/%E6%8A%80%E6%9C%AF%E9%97%AE%E9%A2%98/"}]},{"title":"高级人工智能之通过搜索解决问题","date":"2020-03-20T09:02:14.000Z","path":"2020/03/20/高级人工智能之通过搜索解决问题/","text":"1.人工智能中的问题求解 解：是一个达到目标动作的序列。 过程：寻找该动作，称其为搜索。 问题形式化：给定一个目标，决定要考虑的动作与状态。 为何搜索：对于某些NP完和NP难问题，只能通过搜索来解决。 问题求解智能体：是一种基于目标的智能体，通过搜索来解决问题。 2.相关术语 状态空间：可以形式化地定义为——初始状态、动作和转换模型。 图：状态空间形成一个图，其中节点表示状态、链接表示动作。 路径：状态空间的一条路径是由一系列动作连接的一个状态序列。 3.问题形式化的5个要素 初始状态：智能体出发时的状态。 动作：描述智能体可执行的动作。 转换模型：描述每个动作在做什么。 目标测试：确定一个给定的状态是否为目标状态。 路径代价：每条路径所分配的一个数值代价。 4.搜索算法 一种通用的搜索算法 该frontier(也称为open list)：一种数据结构，用于存储所有的叶节点。 在frontier上扩展结点的过程持续进行，知道找到一个解或者没有其它状态可扩展。 一种通用的图搜索算法 该explored(也称为closed list)：一种数据结构，用于记忆每个扩展结点。 explored和frontier中的结点可以被丢弃。 5.无信息搜索 定义：无信息搜索也被称为盲目搜索。该术语(无信息、盲目的)意味着该搜索策略没有超过问题定义提供的状态之外的附加信息。 所有能做的就是生成后继结点，并且从区分一个目标状态或一个非目标状态。 所有的搜索策略是由节点扩展的顺序加以区分。这些搜索策略是：宽度优先、深度优先以及一致代价搜索。 无信息搜索的策略评价 一种无信息搜索是通过选择结点扩展的顺序来定义的。 其策略可按照如下特性来评价： 完备性。是否总能找到一个存在的解。 时间复杂性：花费多长时间找到这个解。 空间复杂性。需要多少内存。 最优性：是否总能找到最优的解。 时间复杂性和空间复杂性用如下术语来衡量： b–搜索树的最大分支因子。 d–最浅的深度。 m–搜索树的最大深度。 宽度优先搜索 搜索策略：扩展最浅的未扩展节点。 实现方法：使用FIFO(先进先出)队列，即新的后继结点放在后面。 宽度优先搜索不能解决指数复杂性问题，小的分支因子除外。 一致代价搜索 搜索策略：扩展最低代价的未扩展节点。 实现方法：队列，按路径代价排序，最低优先。 深度优先搜索 搜索策略：扩展最深未扩展节点。 实现方法：使用LIFO队列，把后继节点放在队列的前端。 -深度受限搜索 若状态空间无限，深度优先搜索就会发生失败，这个问题可以用一个预定的深度限制得到解决。 缺点： 如果我们选择l &lt; d,即最浅的的目标在深度限制之外，这种方法就会出现额外的不完备性。 如果我们选择l &gt; d,深度受限搜索也不是最优的。 -迭代加深搜索 它将深度优先和宽度优先的优势相结合，逐步增加深度限制反复运行直到找到目标。 它以深度优先搜索相同的顺序访问搜索树的节点，但先访问节点的累积顺序实际是宽度优先。 -双向搜索 它同时进行两个搜索：一个是从初始状态向前搜索，而另一个则从目标向后搜索。当两者在中间相遇时停止。 该方法可以通过一种剩余距离的启发式估计来导向。 -无信息搜索树策略评价 6.有信息搜索 有信息搜索也被称为启发式搜索，这类策略采用超出问题本身定义的、问题特有的知识，因此能够找到比无信息搜索更有效的解。 一般方法使用如下函数的一个或两者： 评价函数，记作f(n)，用于选择一个节点进行扩展。 启发式函数，记作h(n)，作为f的一个组成部分。 -最佳优先搜索 搜索策略：一个节点被选择进行扩展是基于一个评价函数，f(n)。大多数的最佳优先搜索算法还包含一个启发式函数，h(n)。 实现方法：与一致代价搜索相同。然而，最佳优先搜索使用f(n)代替g(n)来整体优先队列。 启发式函数h(n)：从节点n到目标状态的最低路径估计代价。 特例：贪婪搜索、A*搜索 贪婪搜索 搜索策略：试图扩展最接近目标的节点。 评价函数：f(n) = h(n) 它仅使用启发式函数对节点进行评价。 迭代加深A搜索* 它是迭代加深深度优先搜索的变种，从A*搜索算法借鉴了这一思想，即使用启发式函数来评价到目标的剩余代价。 它是一种深度优先搜索算法，内存使用率低于A*算法。但是，不同于标准的迭代加深搜索，它集中于探索最有希望的节点，因此不会去搜索树任何处的同样深度。 比较： 迭代加深深度优先搜索：使用搜索深度作为每次迭代的截止值。 迭代加深A*搜索：使用信息更丰富的评价函数，f(n) = g(n) + h(n) g(n)：到达该节点的代价 h(n)：该节点到目标的估计代价","tags":[{"name":"AI","slug":"AI","permalink":"https://www.musicpoet.top/tags/AI/"}]},{"title":"高级人工智能之智能Agent","date":"2020-03-20T02:09:26.000Z","path":"2020/03/20/高级人工智能之智能Agent/","text":"1.智能Agent的特点： 可自主操作、感知环境、持续动作、顺应变化、实现目标和最佳结果(或者最佳预期结果)。 概括地说，一个智能体可以被看作具有如下功能的任何事物： 通过感受器感知外部环境，并且通过执行器作用于外部环境。 可以通过学习或者应用知识来实现其目标。 2.理性Agent 定义：是一个有正确行为的智能体——该功能表中的每个条目都正确填写。 对于正确行为的解释： 一个智能体在一个环境中依据感知生成一系列的动作。 这些动作由一系列状态而引起环境发生变化。 如果该系列变化是所期望的，则该智能体表现良好。 对于理性的理解： 理性是指探索、学习和自主的。 理性的动作是指对给定的感知序列，能使期待的性能指标最大化。 理性同样依赖于四件事： 定义成功标准的性能指标。 智能体对环境的先验知识。 智能体所能够完成的动作。 智能体最新的感知序列。 3.PEAS 定义：PEAS是一种任务环境的规范，四个大写字母的缩写分别代表Performance(性能)、Environment(环境)、Actuators(动作器)和Sensors(感受器)。 不同的环境类型： 完全可观测和部分可观测。一个智能体的传感器在每个时间点上可访问环境的完整状态，则该任务环境就是完全可观测的。 单智能体和多智能体。一个智能体在一个环境内自运行，则它就是一个单智能体。 确定性和随机性。环境的下一个状态完全由当前的状态和该智能体执行的动作所决定，则该环境是确定的。 阵发性和连续性。智能体的动作过程被分为原子的片段，并且每个片段的动作选择仅仅依赖于片段本身。 动态与静态。如果环境随智能体的行为而改变，则该智能体的环境是动态的；否则是静态的。 离散与连续。离散与连续的区别在于环境的状态、时间处理的方式、以及感知和智能体的动作。 已知与未知。在一个已知的环境下，所有动作的结果都是给定的。如果环境是未知的，则该智能体将需要学习如何动作，以便于做出正确的决策。 4.智能体的结构 智能体的结构如下图所示： 智能体的层次：智能体通常表现为一个分层的结构，它包含许多&quot;子智能体&quot;，而子智能体处理和执行较低级的功能。智能体和子智能体构建一个完整的系统，它可以通过行为和反应来完成艰巨的任务。 表示智能体状态的三种方式： 原子式：每个状态是一个黑盒子，没有内部结构。 因子式：每个状态由一组固定的属性和值组成。 结构式：每个状态包含对象，每个具有属性以及与其他对象的关系。 5.智能体的主要类别 下面介绍的5种类型的智能体，体现几乎所有智能系统的基本原理。 简单反射智能体。 基于模型的反射智能体。 基于目标的智能体。 基于效用的智能体。 学习智能体。 简单反射智能体 简单反射智能体仅仅在当前感知的基础上动作，忽略其余的感知历史。 智能体功能是基于条件动作规则：if 条件 then 动作。 关于简单反射智能体： 仅当外部环境为完全可测时，该智能体的功能才能发挥。 某些反射智能体也可以包含关于其当前状态的信息，允许它们忽视执行器已被触发的条件。 智能体在部分可观测的环境下运行时，无限循环往往是无法避免的。 如果智能体可以随机产生其动作，有可能从无限循环中摆脱出来。 模型反射智能体 一个基于模型的反射智能体可以处理部分可观测环境。 其当前状态存储在智能体中，维护某种结构，它描述不可见外部环境中的一部分。 关于基于模型反射智能体： 关于&quot;外部环境如何运作&quot;的知识被称为一个外部环境模型，由此得名&quot;基于模型的智能体&quot;。 基于模型的反射智能体将保持某种内部模型。 内部模型依赖于感知的历史，因此至少反射某些当前状态无法观测的方面。 它作为反射智能体以某种方式选择动作。 一个基于模型的反射智能体算法。它采用一个内部模型来保持当前外部环境状态的轨迹，然后用等同于简单反射智能体的方式选择一个动作。 基于目标智能体 通过利用&quot;目标&quot;信息，基于目标的智能体进一步扩展了基于模型的智能体的功能。 关于基于目标智能体： 目标信息描述所希望的情形。 它允许智能体在多个可能性之间选择一种方式，挑选出达到目标状态的那一个。 搜索和规划是人工智能的子领域，致力于发现达到智能体目标的动作序列。 在某些情况下，基于目标的智能体似乎不太有效，但是它更为灵活，因为这种支持其决策的知识明显地展示出来，并且可以被修改。 基于效用的智能体 一个特殊的状态可通过一个效用函数得到，该函数将一个状态隐射到该状态效用的度量。 关于基于效用的智能体： 一种更通用的性能度量，应该根据它们使得智能体多么&quot;高兴&quot;的程度，允许对不同的外部环境状态进行比较。 效用这个术语，可用于描述智能体是多么高兴。 一个理性的基于效用的智能体选择动作，将动作的期待效应最大化。 一个基于效用的智能体需要建模并记录环境、任务轨迹，这涉及大量的感知、表征、推理和学习的研究。 学习智能体 学习允许智能体最初在未知的环境中运行，并且与其最初的知识相比，会变得越来越胜任。 关于学习智能体： 学习要素：它利用评论者对智能体如何动作的反馈，然后决定应该如何修改性能要素以便于未来做的更好。 性能要素：获得感知并决定动作。 问题发生器：它对推荐的动作负责，这将形成新的经验。 智能体的分类法","tags":[{"name":"AI","slug":"AI","permalink":"https://www.musicpoet.top/tags/AI/"}]},{"title":"创建型设计模式","date":"2020-03-16T12:36:04.000Z","path":"2020/03/16/创建型设计模式/","text":"一. 创建型模式的特点与分类 创建型模式的主要关注点是“怎么创建对象”，它的主要特点是“将对象的创建与使用分离”。这样可以降低系统的耦合度，使用者不需要关注对象的创建细节，对象的创建由相关的工厂来完成。 创建型模式主要分为以下几种。 单例模式：某个类只生成一个实例，该类提供一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型模式：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法模式：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂模式：提供一个创建产品族的接口，其每个子类可以产生一系列相关的产品。 建造者模式：将一个复杂的对象分解成多个相对简单的部分，然后根据不同的需要分别创建它们，最后构建该复杂对象。 以上 5 种创建型模式，除了工厂方法模式属于类创建型模式，其他的全部属于对象创建型模式。 二.单例模式 1.单例模式的定义与特点 定义：指一个类只有一个实例，且该类能自行创建这个实例的一种模式。 单例模式有3个特点： 单例类只有一个实例对象。 该单例对象必须由单例类自行创建。 单例类对外提供一个访问该单例的全局访问点。 2.单例模式的结构与应用场景 单例模式的主要角色如下： 单例类：包含一个实例且能自行创建这个实例的类。 访问类：使用单例的类 应用场景： 某类只要求生成一个对象的时候。 当对象需要被共享的场合。由于单例模式只允许创建一个对象，共享该对象可以节省内存，并加快对象访问速度。 当某类需要频繁实例化，而创建的对象又频繁被销毁的时候。 三.原型模式 1.原型模式的定义与特点 定义：用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型相同或者相似的新对象。 特点：在这里，原型实例指定了要创建的对象的种类。用这种方法创建对象非常高效，根本不需要知道创建的细节。 2.原型模式的结构与应用场景 原型模式的主要角色如下： 抽象原型类：规定具体原型对象必须实现的接口。 具体原型类：实现抽象原型类的clone()方法，它是可被复制的对象。 访问类：使用具体原型类中的clone()方法来复制新的对象。 应用场景： 对象之间相同或者相似，即只是个别的几个属性不同的时候。 对象的创建过程比较麻烦，但复制比较简单的时候。 四.工厂方法模式 1.工厂方法模式的定义与特点 定义：定义一个创建产品对象的工厂接口，将产品对象实际创建工作推迟到具体子工厂类当中。满足创建型模式中要求“创建与使用相分离“的特点。 我们把创建的对象称为”产品“，把创建产品的对象称为”工厂“。如果要创建的产品不多，只需要一个工厂类就可以完成，这种模式叫”简单工厂模式“，它的缺点是增加新产品时会违背”开闭原则“。 工厂方法模式的主要优点： 用户只需要知道具体工厂的名称就可得到所要的产品，而不需要知道产品的具体创建过程。 在系统增加新的产品时，只需要添加具体产品类和对应的具体工厂类，无须对原工厂进行任何修改，满足开闭原则。 缺点： 每增加一个产品就要增加一个具体的产品类和一个对应的具体工厂类，增加了系统的复杂度。 2.工厂方法模式的结构与应用场景 工厂方法模式的主要角色如下： 抽象工厂：提供了创建产品的接口，调用者通过它访问具体工厂的工厂方法来创建产品。 具体工厂：主要是实现抽象工厂中的抽象方法，完成具体产品的创建。 抽象产品：定义了产品的规范，描述了产品的主要特性和功能。 具体产品：定义抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间一一对应。 应用场景： 客户只知道创建产品的工厂名，而不知道具体的产品名。 创建对象的任务由多个具体子工厂的某一个完成，而抽象工厂只提供创建产品的接口。 客户不关系创建产品的细节，只关心产品的品牌。 五.抽象工厂模式 1.抽象工厂模式的定义与特点 定义：是一种为访问类提供一个创建一组相关或相互依赖对象的接口，且访问类不需要指定所要产品的具体类就能得到同族的不同等级的产品的模式结构。 抽象工厂模式是工厂方法模式的升级版，工厂方法模式只生产一个等级的产品，而抽象工厂模式可生产多个等级的产品。 使用抽象工厂模式一般要满足以下条件： 系统中有多个产品族，每个具体工厂创建同一族但属于不同等级结构的产品。 系统一次只可能消费其中某一族的产品，即同族的产品一起使用。 抽象工厂模式的主要优点： 可以在类的内部对产品族中相关联的多等级产品共同管理，而不必专门引入多个新的类来进行管理。 当增加一个新的产品族时不需要修改源代码，满足开闭原则。 缺点： 当产品族中需要增加一个新产品时，所有的工厂类都需要进行修改。 2.抽象工厂模式的结构与应用场景 抽象工厂模式的主要角色如下： 抽象工厂：提供了创建产品的接口，它包含多个创建产品的方法 ，可以创建多个不同等级的产品。 具体工厂：主要是实现抽象工厂中的多个抽象方法，完成具体产品的创建。 抽象产品：定义了产品的规范，描述了产品的主要特性和功能，抽象工厂模式有多个抽象产品。 具体产品：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间是多对一的关系。 应用场景： 当需要创建的对象是一系列相互关联或相互依赖的产品族时。 系统中有多个产品族，但每次只使用其中的某一族产品。 系统中提供了产品的类库，且所有产品的接口相同，客户端不依赖产品实例的创建细节和内部结构。 六.建造者模式 1.建造者模式的定义与特点 定义：指将一个复杂对象的构造与它的表示分离，使同样的构建过程可以创建不同的表示，这样的设计模式被称为”建造者模式“。它将一个复杂的对象分解为多个简单的对象，然后一步步构建而成，它将变与不变相分离，即产品的组成部分是不变的，但每一部分是可以灵活选择的。 建造者模式的主要优点： 各个具体的建造者相互独立，有利于系统的扩展。 客户端不必知道产品内部组成的细节，便于控制细节风险。 缺点： 产品的组成部分必须相同，这限制了其使用范围。 如果产品的内部变化复杂，该模式会增加很多建造者类。 建造者模式和工厂模式的关注点不同：建造者模式注重零部件的组装过程，而工厂模式更注重零部件的创建过程，但两者可以结合使用。 2.建造者模式的结构与应用场景 建造者模式的主要角色如下： 产品角色：它是包含多个组成部件的复杂对象，由具体建造者来创建其各个滅部件。 抽象建造者：它是一个包含创建产品各个子部件的抽象方法的接口，通常还包含一个返回复杂产品的方法 。 具体建造者：实现 Builder 接口，完成复杂产品的各个部件的具体创建方法。 指挥者：它调用建造者对象中的部件构造与装配方法完成复杂对象的创建，在指挥者中不涉及具体产品的信息。 应用场景： 创建的对象较复杂，由多个部件构成，各个部件面临着复杂的变化，但构件间的建造顺序是稳定。 创建复杂对象的算法独立于该对象的组成部分以及它们的装配方式，即产品的构建过程和最终的表示是独立的。 以上是对5种创建型模式的简单介绍，文章整理自C语言中心网 ,感谢您的阅读。","tags":[{"name":"设计模式 面向对象编程","slug":"设计模式-面向对象编程","permalink":"https://www.musicpoet.top/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B/"}]}]